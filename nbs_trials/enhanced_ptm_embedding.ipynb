{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thread B: Enhanced PTM Embedding\n",
    "\n",
    "This notebook implements an enhanced PTM embedding strategy that integrates additional chemical descriptors to improve spectral prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peptdeep.pretrained_models import ModelManager\n",
    "from peptdeep.model.featurize import get_batch_mod_feature\n",
    "from peptdeep.settings import model_const, MOD_DF, MOD_TO_FEATURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Current PTM Embedding Approach\n",
    "\n",
    "Let's first examine the current PTM embedding approach in AlphaPeptDeep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "model_mgr = ModelManager(mask_modloss=False, device='cpu')\n",
    "model_mgr.load_installed_models('phos')\n",
    "\n",
    "# Print the current PTM feature dimensions\n",
    "print(f\"Current PTM feature dimensions: {len(model_const['mod_elements'])}\")\n",
    "print(f\"Current mod elements: {model_const['mod_elements']}\")\n",
    "\n",
    "# Print some example PTM features\n",
    "print(\"\\nExample PTM features:\")\n",
    "for mod_name, feature in list(MOD_TO_FEATURE.items())[:5]:\n",
    "    print(f\"{mod_name}: {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced PTM Embedding\n",
    "\n",
    "We'll extend the current PTM embedding with additional chemical descriptors such as:\n",
    "1. Molecular weight\n",
    "2. Hydrophobicity\n",
    "3. Polarity\n",
    "4. Charge\n",
    "5. Size\n",
    "\n",
    "First, let's create a dictionary of these properties for common PTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional chemical properties for common PTMs\n",
    "ptm_chemical_properties = {\n",
    "    'Phospho@S': {\n",
    "        'molecular_weight': 79.97,  # Da\n",
    "        'hydrophobicity': -3.5,     # Relative scale\n",
    "        'polarity': 4.0,            # Relative scale\n",
    "        'charge': -2.0,             # At pH 7\n",
    "        'size': 1.2                 # Relative scale\n",
    "    },\n",
    "    'Phospho@T': {\n",
    "        'molecular_weight': 79.97,\n",
    "        'hydrophobicity': -3.5,\n",
    "        'polarity': 4.0,\n",
    "        'charge': -2.0,\n",
    "        'size': 1.2\n",
    "    },\n",
    "    'Phospho@Y': {\n",
    "        'molecular_weight': 79.97,\n",
    "        'hydrophobicity': -3.0,\n",
    "        'polarity': 3.8,\n",
    "        'charge': -2.0,\n",
    "        'size': 1.2\n",
    "    },\n",
    "    'Carbamidomethyl@C': {\n",
    "        'molecular_weight': 57.02,\n",
    "        'hydrophobicity': -0.5,\n",
    "        'polarity': 1.5,\n",
    "        'charge': 0.0,\n",
    "        'size': 0.8\n",
    "    },\n",
    "    'Oxidation@M': {\n",
    "        'molecular_weight': 15.99,\n",
    "        'hydrophobicity': -1.5,\n",
    "        'polarity': 2.0,\n",
    "        'charge': 0.0,\n",
    "        'size': 0.4\n",
    "    },\n",
    "    'Acetyl@Protein_N-term': {\n",
    "        'molecular_weight': 42.01,\n",
    "        'hydrophobicity': 0.0,\n",
    "        'polarity': 1.0,\n",
    "        'charge': -1.0,\n",
    "        'size': 0.6\n",
    "    },\n",
    "    'GlyGly@K': {\n",
    "        'molecular_weight': 114.04,\n",
    "        'hydrophobicity': -0.2,\n",
    "        'polarity': 1.2,\n",
    "        'charge': 0.0,\n",
    "        'size': 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to get default values for PTMs not in our dictionary\n",
    "def get_default_ptm_properties():\n",
    "    return {\n",
    "        'molecular_weight': 0.0,\n",
    "        'hydrophobicity': 0.0,\n",
    "        'polarity': 0.0,\n",
    "        'charge': 0.0,\n",
    "        'size': 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Enhanced PTM Embedding\n",
    "\n",
    "Now, let's implement a function to create enhanced PTM embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_mod_feature(mod_name):\n",
    "    \"\"\"Get enhanced modification feature vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mod_name : str\n",
    "        Modification name\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Enhanced modification feature vector\n",
    "    \"\"\"\n",
    "    # Get the original feature vector\n",
    "    original_feature = MOD_TO_FEATURE.get(mod_name, np.zeros(len(model_const['mod_elements'])))\n",
    "    \n",
    "    # Get chemical properties\n",
    "    properties = ptm_chemical_properties.get(mod_name, get_default_ptm_properties())\n",
    "    \n",
    "    # Normalize properties to [0, 1] range\n",
    "    mw_norm = min(1.0, properties['molecular_weight'] / 200.0)  # Normalize by max expected MW\n",
    "    hydro_norm = (properties['hydrophobicity'] + 4.0) / 8.0     # Range from -4 to 4\n",
    "    polar_norm = properties['polarity'] / 5.0                   # Range from 0 to 5\n",
    "    charge_norm = (properties['charge'] + 2.0) / 4.0            # Range from -2 to 2\n",
    "    size_norm = properties['size'] / 2.0                        # Range from 0 to 2\n",
    "    \n",
    "    # Create enhanced feature vector\n",
    "    enhanced_feature = np.concatenate([\n",
    "        original_feature,\n",
    "        np.array([mw_norm, hydro_norm, polar_norm, charge_norm, size_norm])\n",
    "    ])\n",
    "    \n",
    "    return enhanced_feature\n",
    "\n",
    "# Create enhanced feature vectors for all modifications\n",
    "enhanced_mod_to_feature = {}\n",
    "for mod_name in MOD_TO_FEATURE.keys():\n",
    "    enhanced_mod_to_feature[mod_name] = get_enhanced_mod_feature(mod_name)\n",
    "\n",
    "# Print some examples\n",
    "print(\"Enhanced PTM features:\")\n",
    "for mod_name, feature in list(enhanced_mod_to_feature.items())[:5]:\n",
    "    print(f\"{mod_name}: {feature}\")\n",
    "\n",
    "# Print dimensions\n",
    "print(f\"\\nOriginal feature dimension: {len(next(iter(MOD_TO_FEATURE.values())))}\")\n",
    "print(f\"Enhanced feature dimension: {len(next(iter(enhanced_mod_to_feature.values())))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Attention Mechanism for PTM Features\n",
    "\n",
    "Next, let's implement an attention mechanism to dynamically weight the PTM features based on the peptide sequence context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTMAttentionLayer(torch.nn.Module):\n",
    "    \"\"\"Attention layer for PTM features\"\"\"\n",
    "    \n",
    "    def __init__(self, ptm_dim, hidden_dim):\n",
    "        \"\"\"Initialize the attention layer\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ptm_dim : int\n",
    "            Dimension of PTM features\n",
    "        hidden_dim : int\n",
    "            Dimension of hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(hidden_dim, ptm_dim)\n",
    "        self.key = torch.nn.Linear(ptm_dim, ptm_dim)\n",
    "        self.value = torch.nn.Linear(ptm_dim, ptm_dim)\n",
    "        self.scale = torch.sqrt(torch.tensor(ptm_dim, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, ptm_features, seq_features):\n",
    "        \"\"\"Forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ptm_features : torch.Tensor\n",
    "            PTM features [batch_size, seq_len, ptm_dim]\n",
    "        seq_features : torch.Tensor\n",
    "            Sequence features [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Attended PTM features [batch_size, seq_len, ptm_dim]\n",
    "        \"\"\"\n",
    "        # Generate query from sequence features\n",
    "        q = self.query(seq_features)  # [batch_size, seq_len, ptm_dim]\n",
    "        \n",
    "        # Generate key and value from PTM features\n",
    "        k = self.key(ptm_features)    # [batch_size, seq_len, ptm_dim]\n",
    "        v = self.value(ptm_features)  # [batch_size, seq_len, ptm_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(scores, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        attended_features = torch.matmul(attn_weights, v)  # [batch_size, seq_len, ptm_dim]\n",
    "        \n",
    "        return attended_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modified MS2 Model with Enhanced PTM Embedding\n",
    "\n",
    "Now, let's modify the MS2 model to incorporate our enhanced PTM embedding and attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peptdeep.model.building_block as building_block\n",
    "\n",
    "class EnhancedPTMEmbedding(torch.nn.Module):\n",
    "    \"\"\"Enhanced PTM embedding module\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        \"\"\"Initialize the enhanced PTM embedding module\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_dim : int\n",
    "            Dimension of hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Original PTM dimension + 5 additional chemical properties\n",
    "        self.ptm_dim = len(model_const['mod_elements']) + 5\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.ptm_embedding = torch.nn.Linear(self.ptm_dim, hidden_dim)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = PTMAttentionLayer(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, ptm_features, seq_features):\n",
    "        \"\"\"Forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ptm_features : torch.Tensor\n",
    "            PTM features [batch_size, seq_len, ptm_dim]\n",
    "        seq_features : torch.Tensor\n",
    "            Sequence features [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Combined features [batch_size, seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Embed PTM features\n",
    "        ptm_embedded = self.ptm_embedding(ptm_features)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_ptm = self.attention(ptm_embedded, seq_features)\n",
    "        \n",
    "        # Combine with sequence features\n",
    "        combined = seq_features + attended_ptm\n",
    "        \n",
    "        return combined\n",
    "\n",
    "class EnhancedMS2Model(torch.nn.Module):\n",
    "    \"\"\"MS2 model with enhanced PTM embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, num_frag_types, num_modloss_types=0, mask_modloss=True, dropout=0.1, nlayers=4, hidden=256):\n",
    "        \"\"\"Initialize the enhanced MS2 model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_frag_types : int\n",
    "            Number of fragment types\n",
    "        num_modloss_types : int\n",
    "            Number of modloss types\n",
    "        mask_modloss : bool\n",
    "            Whether to mask modloss\n",
    "        dropout : float\n",
    "            Dropout rate\n",
    "        nlayers : int\n",
    "            Number of transformer layers\n",
    "        hidden : int\n",
    "            Hidden dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self._num_modloss_types = num_modloss_types\n",
    "        self._num_non_modloss = num_frag_types - num_modloss_types\n",
    "        self._mask_modloss = mask_modloss\n",
    "        if num_modloss_types == 0:\n",
    "            self._mask_modloss = True\n",
    "        \n",
    "        meta_dim = 8\n",
    "        self.input_nn = building_block.Input_26AA_Mod_PositionalEncoding(hidden - meta_dim)\n",
    "        \n",
    "        # Enhanced PTM embedding\n",
    "        self.enhanced_ptm_embedding = EnhancedPTMEmbedding(hidden - meta_dim)\n",
    "        \n",
    "        self.meta_nn = building_block.Meta_Embedding(meta_dim)\n",
    "        \n",
    "        self.hidden_nn = building_block.Hidden_HFace_Transformer(hidden, nlayers=nlayers, dropout=dropout)\n",
    "        \n",
    "        self.output_nn = building_block.Decoder_Linear(hidden, self._num_non_modloss)\n",
    "        \n",
    "        if num_modloss_types > 0:\n",
    "            self.modloss_nn = torch.nn.ModuleList([\n",
    "                building_block.Hidden_HFace_Transformer(hidden, nlayers=1, dropout=dropout),\n",
    "                building_block.Decoder_Linear(hidden, num_modloss_types)\n",
    "            ])\n",
    "        else:\n",
    "            self.modloss_nn = None\n",
    "    \n",
    "    def forward(self, aa_indices, mod_x, charges, NCEs, instrument_indices):\n",
    "        \"\"\"Forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        aa_indices : torch.Tensor\n",
    "            Amino acid indices\n",
    "        mod_x : torch.Tensor\n",
    "            Modification features\n",
    "        charges : torch.Tensor\n",
    "            Charges\n",
    "        NCEs : torch.Tensor\n",
    "            NCEs\n",
    "        instrument_indices : torch.Tensor\n",
    "            Instrument indices\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # Get sequence features\n",
    "        seq_features = self.input_nn(aa_indices, None)  # Don't pass mod_x yet\n",
    "        \n",
    "        # Apply enhanced PTM embedding\n",
    "        enhanced_features = self.enhanced_ptm_embedding(mod_x, seq_features)\n",
    "        \n",
    "        # Apply dropout\n",
    "        in_x = self.dropout(enhanced_features)\n",
    "        \n",
    "        # Add metadata\n",
    "        meta_x = self.meta_nn(charges, NCEs, instrument_indices).unsqueeze(1).repeat(1, in_x.size(1), 1)\n",
    "        in_x = torch.cat((in_x, meta_x), 2)\n",
    "        \n",
    "        # Apply transformer\n",
    "        hidden_x = self.hidden_nn(in_x)[0]\n",
    "        hidden_x = self.dropout(hidden_x + in_x * 0.2)\n",
    "        \n",
    "        # Output layer\n",
    "        out_x = self.output_nn(hidden_x)\n",
    "        \n",
    "        # Handle modloss\n",
    "        if self._num_modloss_types > 0:\n",
    "            if self._mask_modloss:\n",
    "                out_x = torch.cat((\n",
    "                    out_x,\n",
    "                    torch.zeros(*out_x.size()[:2], self._num_modloss_types, device=in_x.device)\n",
    "                ), 2)\n",
    "            else:\n",
    "                modloss_x = self.modloss_nn[0](in_x)[0] + hidden_x\n",
    "                modloss_x = self.modloss_nn[-1](modloss_x)\n",
    "                out_x = torch.cat((out_x, modloss_x), 2)\n",
    "        \n",
    "        return out_x[:, 3:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function to Get Enhanced Batch Mod Features\n",
    "\n",
    "Let's implement a function to get enhanced batch modification features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_batch_mod_feature(batch_df):\n",
    "    \"\"\"Get enhanced batch modification features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_df : pd.DataFrame\n",
    "        Batch dataframe\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Enhanced batch modification features\n",
    "    \"\"\"\n",
    "    # Get original batch mod features\n",
    "    original_features = get_batch_mod_feature(batch_df)\n",
    "    \n",
    "    # Create enhanced features\n",
    "    batch_size = len(batch_df)\n",
    "    max_len = max(batch_df.nAA)\n",
    "    enhanced_features = np.zeros((batch_size, max_len, len(model_const['mod_elements']) + 5))\n",
    "    \n",
    "    for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "        seq_len = row.nAA\n",
    "        \n",
    "        # Copy original features\n",
    "        enhanced_features[i, :seq_len, :len(model_const['mod_elements'])] = original_features[i, :seq_len]\n",
    "        \n",
    "        # Add chemical properties\n",
    "        if row.mods and row.mod_sites:\n",
    "            mods = row.mods.split(';')\n",
    "            mod_sites = [int(site) for site in row.mod_sites.split(';')]\n",
    "            \n",
    "            for mod, site in zip(mods, mod_sites):\n",
    "                if site < seq_len and mod in ptm_chemical_properties:\n",
    "                    props = ptm_chemical_properties[mod]\n",
    "                    \n",
    "                    # Normalize properties\n",
    "                    mw_norm = min(1.0, props['molecular_weight'] / 200.0)\n",
    "                    hydro_norm = (props['hydrophobicity'] + 4.0) / 8.0\n",
    "                    polar_norm = props['polarity'] / 5.0\n",
    "                    charge_norm = (props['charge'] + 2.0) / 4.0\n",
    "                    size_norm = props['size'] / 2.0\n",
    "                    \n",
    "                    # Add to features\n",
    "                    enhanced_features[i, site, len(model_const['mod_elements'])] = mw_norm\n",
    "                    enhanced_features[i, site, len(model_const['mod_elements'])+1] = hydro_norm\n",
    "                    enhanced_features[i, site, len(model_const['mod_elements'])+2] = polar_norm\n",
    "                    enhanced_features[i, site, len(model_const['mod_elements'])+3] = charge_norm\n",
    "                    enhanced_features[i, site, len(model_const['mod_elements'])+4] = size_norm\n",
    "    \n",
    "    return torch.tensor(enhanced_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the Enhanced PTM Embedding\n",
    "\n",
    "Let's create a small dataset to test our enhanced PTM embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset with phosphorylated peptides\n",
    "test_df = pd.DataFrame({\n",
    "    'sequence': ['ANEKTESSSAQQVAVSR', 'ANEKTESSTAQQVAVSR', 'ANEKTESSYAQQVAVSR', 'ANEKTESSSAQQVAVSR'],\n",
    "    'mods': ['Phospho@S', 'Phospho@T', 'Phospho@Y', ''],\n",
    "    'mod_sites': ['9', '9', '9', ''],\n",
    "    'charge': [2, 2, 2, 2],\n",
    "    'nce': [30, 30, 30, 30],\n",
    "    'instrument': ['QE', 'QE', 'QE', 'QE']\n",
    "})\n",
    "test_df['nAA'] = test_df.sequence.str.len()\n",
    "\n",
    "# Get enhanced batch mod features\n",
    "enhanced_features = get_enhanced_batch_mod_feature(test_df)\n",
    "\n",
    "print(f\"Enhanced features shape: {enhanced_features.shape}\")\n",
    "\n",
    "# Print the enhanced features for the phosphorylated site\n",
    "print(\"\\nEnhanced features for phosphorylated sites:\")\n",
    "for i, row in test_df.iterrows():\n",
    "    if row.mods:\n",
    "        site = int(row.mod_sites)\n",
    "        print(f\"{row.sequence[site]}{site+1} with {row.mods}: {enhanced_features[i, site].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration with MS2 Prediction Pipeline\n",
    "\n",
    "Now, let's integrate our enhanced PTM embedding into the MS2 prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our enhanced MS2 model\n",
    "from peptdeep.model.ms2 import pDeepModel\n",
    "from alphabase.peptide.fragment import get_charged_frag_types\n",
    "from peptdeep.settings import global_settings\n",
    "\n",
    "# Get fragment types\n",
    "frag_types = global_settings[\"model\"][\"frag_types\"]\n",
    "max_frag_charge = global_settings[\"model\"][\"max_frag_charge\"]\n",
    "charged_frag_types = get_charged_frag_types(frag_types, max_frag_charge)\n",
    "\n",
    "# Create a custom MS2 model with enhanced PTM embedding\n",
    "class EnhancedPDeepModel(pDeepModel):\n",
    "    \"\"\"pDeepModel with enhanced PTM embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, charged_frag_types=charged_frag_types, dropout=0.1, mask_modloss=False, device=\"cpu\"):\n",
    "        \"\"\"Initialize the enhanced pDeepModel\"\"\"\n",
    "        super().__init__(charged_frag_types=charged_frag_types, dropout=dropout, mask_modloss=mask_modloss, device=device)\n",
    "        \n",
    "        # Replace the model with our enhanced model\n",
    "        self.model = EnhancedMS2Model(\n",
