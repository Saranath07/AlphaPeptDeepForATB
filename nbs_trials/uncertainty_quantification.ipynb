{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thread A: Uncertainty Quantification and Interpretability\n",
    "\n",
    "This notebook implements uncertainty quantification for AlphaPeptDeep models using two approaches:\n",
    "1. Monte Carlo Dropout\n",
    "2. Deep Ensembles\n",
    "\n",
    "We'll evaluate the uncertainty estimates using Prediction Interval Coverage Probability (PICP) and calibration curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peptdeep.pretrained_models import ModelManager\n",
    "from peptdeep.model.rt import IRT_PEPTIDE_DF\n",
    "from peptdeep.utils import evaluate_linear_regression, evaluate_linear_regression_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "model_mgr = ModelManager(mask_modloss=False, device='cpu')\n",
    "model_mgr.load_installed_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Dropout Implementation\n",
    "\n",
    "Monte Carlo Dropout involves enabling dropout at inference time to sample from the approximate posterior distribution of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutPredictor:\n",
    "    \"\"\"Monte Carlo Dropout predictor for uncertainty quantification\"\"\"\n",
    "    \n",
    "    def __init__(self, model, n_samples=30):\n",
    "        \"\"\"Initialize the MC Dropout predictor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : ModelInterface\n",
    "            The model to use for prediction\n",
    "        n_samples : int\n",
    "            Number of Monte Carlo samples to draw\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "    def enable_dropout(self, model):\n",
    "        \"\"\"Enable dropout during inference\"\"\"\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, torch.nn.Dropout):\n",
    "                module.train()\n",
    "                \n",
    "    def predict(self, df, batch_size=1024):\n",
    "        \"\"\"Predict with uncertainty using Monte Carlo Dropout\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame with peptide sequences\n",
    "        batch_size : int\n",
    "            Batch size for prediction\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with predictions and uncertainty estimates\n",
    "        \"\"\"\n",
    "        # Store original model state\n",
    "        was_training = self.model.model.training\n",
    "        \n",
    "        # Set model to eval mode but enable dropout\n",
    "        self.model.model.eval()\n",
    "        self.enable_dropout(self.model.model)\n",
    "        \n",
    "        # Collect predictions\n",
    "        predictions = []\n",
    "        for _ in range(self.n_samples):\n",
    "            pred_df = self.model.predict(df, batch_size=batch_size)\n",
    "            if hasattr(self.model, 'target_column_to_predict'):\n",
    "                pred_col = self.model.target_column_to_predict\n",
    "            else:\n",
    "                # For RT model\n",
    "                pred_col = 'rt_pred'\n",
    "            predictions.append(pred_df[pred_col].values)\n",
    "        \n",
    "        # Restore original model state\n",
    "        self.model.model.train(was_training)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        predictions = np.array(predictions)\n",
    "        mean = np.mean(predictions, axis=0)\n",
    "        std = np.std(predictions, axis=0)\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        result_df = df.copy()\n",
    "        result_df[f'{pred_col}_mean'] = mean\n",
    "        result_df[f'{pred_col}_std'] = std\n",
    "        result_df[f'{pred_col}_lower'] = mean - 1.96 * std  # 95% confidence interval\n",
    "        result_df[f'{pred_col}_upper'] = mean + 1.96 * std  # 95% confidence interval\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Ensembles Implementation\n",
    "\n",
    "Deep Ensembles involve training multiple models with different random initializations and aggregating their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepEnsemblePredictor:\n",
    "    \"\"\"Deep Ensemble predictor for uncertainty quantification\"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, n_models=5, **model_kwargs):\n",
    "        \"\"\"Initialize the Deep Ensemble predictor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model_class : class\n",
    "            The model class to instantiate\n",
    "        n_models : int\n",
    "            Number of models in the ensemble\n",
    "        model_kwargs : dict\n",
    "            Keyword arguments for model initialization\n",
    "        \"\"\"\n",
    "        self.models = [model_class(**model_kwargs) for _ in range(n_models)]\n",
    "        self.n_models = n_models\n",
    "        \n",
    "    def train(self, df, **train_kwargs):\n",
    "        \"\"\"Train all models in the ensemble\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Training data\n",
    "        train_kwargs : dict\n",
    "            Keyword arguments for training\n",
    "        \"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"Training model {i+1}/{self.n_models}\")\n",
    "            model.train(df, **train_kwargs)\n",
    "    \n",
    "    def predict(self, df, batch_size=1024):\n",
    "        \"\"\"Predict with uncertainty using Deep Ensemble\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame with peptide sequences\n",
    "        batch_size : int\n",
    "            Batch size for prediction\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with predictions and uncertainty estimates\n",
    "        \"\"\"\n",
    "        # Collect predictions from all models\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            pred_df = model.predict(df, batch_size=batch_size)\n",
    "            if hasattr(model, 'target_column_to_predict'):\n",
    "                pred_col = model.target_column_to_predict\n",
    "            else:\n",
    "                # For RT model\n",
    "                pred_col = 'rt_pred'\n",
    "            predictions.append(pred_df[pred_col].values)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        predictions = np.array(predictions)\n",
    "        mean = np.mean(predictions, axis=0)\n",
    "        std = np.std(predictions, axis=0)\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        result_df = df.copy()\n",
    "        result_df[f'{pred_col}_mean'] = mean\n",
    "        result_df[f'{pred_col}_std'] = std\n",
    "        result_df[f'{pred_col}_lower'] = mean - 1.96 * std  # 95% confidence interval\n",
    "        result_df[f'{pred_col}_upper'] = mean + 1.96 * std  # 95% confidence interval\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics\n",
    "\n",
    "We'll implement metrics to evaluate the quality of uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_picp(y_true, y_lower, y_upper):\n",
    "    \"\"\"Calculate Prediction Interval Coverage Probability (PICP)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    y_lower : array-like\n",
    "        Lower bounds of prediction intervals\n",
    "    y_upper : array-like\n",
    "        Upper bounds of prediction intervals\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        PICP value (between 0 and 1)\n",
    "    \"\"\"\n",
    "    covered = np.logical_and(y_true >= y_lower, y_true <= y_upper)\n",
    "    return np.mean(covered)\n",
    "\n",
    "def calculate_mpiw(y_lower, y_upper):\n",
    "    \"\"\"Calculate Mean Prediction Interval Width (MPIW)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_lower : array-like\n",
    "        Lower bounds of prediction intervals\n",
    "    y_upper : array-like\n",
    "        Upper bounds of prediction intervals\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        MPIW value\n",
    "    \"\"\"\n",
    "    return np.mean(y_upper - y_lower)\n",
    "\n",
    "def plot_calibration_curve(y_true, y_pred, y_std, n_bins=10):\n",
    "    \"\"\"Plot calibration curve for uncertainty estimates\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    y_std : array-like\n",
    "        Standard deviations of predictions\n",
    "    n_bins : int\n",
    "        Number of bins for the calibration curve\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        Figure with calibration curve\n",
    "    \"\"\"\n",
    "    # Calculate normalized errors\n",
    "    normalized_errors = np.abs(y_true - y_pred) / y_std\n",
    "    \n",
    "    # Create bins and calculate frequencies\n",
    "    bins = np.linspace(0, 3, n_bins+1)  # Up to 3 standard deviations\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    observed_freq = np.zeros(n_bins)\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        observed_freq[i] = np.mean(normalized_errors <= bins[i+1])\n",
    "    \n",
    "    # Expected frequencies for a Gaussian distribution\n",
    "    expected_freq = np.array([norm.cdf(b) for b in bins[1:]])\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(expected_freq, observed_freq, 'o-', label='Calibration curve')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Ideal calibration')\n",
    "    ax.set_xlabel('Expected cumulative probability')\n",
    "    ax.set_ylabel('Observed cumulative probability')\n",
    "    ax.set_title('Calibration Curve for Uncertainty Estimates')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment with RT Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a dataset for RT prediction\n",
    "# For this example, we'll use the IRT_PEPTIDE_DF dataset\n",
    "df = IRT_PEPTIDE_DF.copy()\n",
    "df['rt_norm'] = df['irt']  # Use iRT values as normalized RT\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Monte Carlo Dropout for RT Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MC Dropout predictor for RT model\n",
    "mc_dropout_rt = MCDropoutPredictor(model_mgr.rt_model, n_samples=30)\n",
    "\n",
    "# Predict with uncertainty\n",
    "mc_results = mc_dropout_rt.predict(test_df)\n",
    "\n",
    "# Evaluate\n",
    "picp = calculate_picp(mc_results['rt_norm'], mc_results['rt_pred_lower'], mc_results['rt_pred_upper'])\n",
    "mpiw = calculate_mpiw(mc_results['rt_pred_lower'], mc_results['rt_pred_upper'])\n",
    "\n",
    "print(f\"Monte Carlo Dropout - PICP: {picp:.4f}, MPIW: {mpiw:.4f}\")\n",
    "\n",
    "# Plot calibration curve\n",
    "fig = plot_calibration_curve(mc_results['rt_norm'], mc_results['rt_pred_mean'], mc_results['rt_pred_std'])\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions with uncertainty\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(mc_results['rt_norm'], mc_results['rt_pred_mean'], \n",
    "             yerr=1.96*mc_results['rt_pred_std'], fmt='o', alpha=0.5)\n",
    "plt.plot([min(mc_results['rt_norm']), max(mc_results['rt_norm'])], \n",
    "         [min(mc_results['rt_norm']), max(mc_results['rt_norm'])], 'k--')\n",
    "plt.xlabel('True RT')\n",
    "plt.ylabel('Predicted RT')\n",
    "plt.title('RT Prediction with Uncertainty (Monte Carlo Dropout)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Deep Ensembles for RT Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RT model class\n",
    "from peptdeep.model.rt import AlphaRTModel\n",
    "\n",
    "# Create Deep Ensemble predictor for RT model\n",
    "ensemble_rt = DeepEnsemblePredictor(AlphaRTModel, n_models=5, device='cpu')\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble_rt.train(train_df, epoch=10, batch_size=32, verbose=True)\n",
    "\n",
    "# Predict with uncertainty\n",
    "ensemble_results = ensemble_rt.predict(test_df)\n",
    "\n",
    "# Evaluate\n",
    "picp = calculate_picp(ensemble_results['rt_norm'], ensemble_results['rt_pred_lower'], ensemble_results['rt_pred_upper'])\n",
    "mpiw = calculate_mpiw(ensemble_results['rt_pred_lower'], ensemble_results['rt_pred_upper'])\n",
    "\n",
    "print(f\"Deep Ensemble - PICP: {picp:.4f}, MPIW: {mpiw:.4f}\")\n",
    "\n",
    "# Plot calibration curve\n",
    "fig = plot_calibration_curve(ensemble_results['rt_norm'], ensemble_results['rt_pred_mean'], ensemble_results['rt_pred_std'])\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions with uncertainty\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(ensemble_results['rt_norm'], ensemble_results['rt_pred_mean'], \n",
    "             yerr=1.96*ensemble_results['rt_pred_std'], fmt='o', alpha=0.5)\n",
    "plt.plot([min(ensemble_results['rt_norm']), max(ensemble_results['rt_norm'])], \n",
    "         [min(ensemble_results['rt_norm']), max(ensemble_results['rt_norm'])], 'k--')\n",
    "plt.xlabel('True RT')\n",
    "plt.ylabel('Predicted RT')\n",
    "plt.title('RT Prediction with Uncertainty (Deep Ensemble)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment with MS2 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small dataset for MS2 prediction\n",
    "ms2_df = pd.DataFrame({\n",
    "    'sequence': ['LGGNEQVTR', 'GAGSSEPVTGLDAK', 'VEATFGVDESNAK', 'YILAGVENSK'],\n",
    "    'mods': ['', '', '', ''],\n",
    "    'mod_sites': ['', '', '', ''],\n",
    "    'charge': [2, 2, 2, 2],\n",
    "    'nce': [30, 30, 30, 30],\n",
    "    'instrument': ['QE', 'QE', 'QE', 'QE']\n",
    "})\n",
    "ms2_df['nAA'] = ms2_df.sequence.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Monte Carlo Dropout for MS2 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MC Dropout predictor for MS2 model\n",
    "# For MS2, we need to modify our approach since the output is a fragment intensity dataframe\n",
    "\n",
    "def predict_ms2_with_uncertainty(model_mgr, df, n_samples=30):\n",
    "    \"\"\"Predict MS2 spectra with uncertainty using Monte Carlo Dropout\"\"\"\n",
    "    # Store original model state\n",
    "    was_training = model_mgr.ms2_model.model.training\n",
    "    \n",
    "    # Set model to eval mode but enable dropout\n",
    "    model_mgr.ms2_model.model.eval()\n",
    "    for module in model_mgr.ms2_model.model.modules():\n",
    "        if isinstance(module, torch.nn.Dropout):\n",
    "            module.train()\n",
    "    \n",
    "    # Prepare data for prediction\n",
    "    from alphabase.peptide.fragment import init_fragment_by_precursor_dataframe\n",
    "    init_fragment_by_precursor_dataframe(df, model_mgr.ms2_model.charged_frag_types)\n",
    "    \n",
    "    # Collect predictions\n",
    "    all_intensities = []\n",
    "    for _ in range(n_samples):\n",
    "        result = model_mgr.predict_all(df.copy(), predict_items=['ms2'])\n",
    "        all_intensities.append(result['fragment_intensity_df'].values)\n",
    "    \n",
    "    # Restore original model state\n",
    "    model_mgr.ms2_model.model.train(was_training)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    all_intensities = np.array(all_intensities)\n",
    "    mean_intensities = np.mean(all_intensities, axis=0)\n",
    "    std_intensities = np.std(all_intensities, axis=0)\n",
    "    \n",
    "    # Create result DataFrames\n",
    "    mean_df = pd.DataFrame(mean_intensities, columns=result['fragment_intensity_df'].columns)\n",
    "    std_df = pd.DataFrame(std_intensities, columns=result['fragment_intensity_df'].columns)\n",
    "    \n",
    "    return {\n",
    "        'precursor_df': df,\n",
    "        'fragment_mz_df': result['fragment_mz_df'],\n",
    "        'fragment_intensity_mean_df': mean_df,\n",
    "        'fragment_intensity_std_df': std_df\n",
    "    }\n",
    "\n",
    "# Predict MS2 with uncertainty\n",
    "ms2_results = predict_ms2_with_uncertainty(model_mgr, ms2_df, n_samples=10)\n",
    "\n",
    "# Plot MS2 spectrum with uncertainty for the first peptide\n",
    "peptide_idx = 0\n",
    "start_idx = ms2_df.iloc[peptide_idx]['frag_start_idx']\n",
    "end_idx = ms2_df.iloc[peptide_idx]['frag_stop_idx']\n",
    "\n",
    "# Get fragment m/z and intensities for this peptide\n",
    "frag_mz = ms2_results['fragment_mz_df'].iloc[start_idx:end_idx]\n",
    "frag_intensity_mean = ms2_results['fragment_intensity_mean_df'].iloc[start_idx:end_idx]\n",
    "frag_intensity_std = ms2_results['fragment_intensity_std_df'].iloc[start_idx:end_idx]\n",
    "\n",
    "# Plot for b and y ions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot b ions\n",
    "b_cols = [col for col in frag_mz.columns if col.startswith('b_')]\n",
    "for col in b_cols:\n",
    "    mz_values = frag_mz[col].values\n",
    "    intensity_values = frag_intensity_mean[col].values\n",
    "    std_values = frag_intensity_std[col].values\n",
    "    \n",
    "    # Filter out zero m/z values\n",
    "    mask = mz_values > 0\n",
    "    mz_values = mz_values[mask]\n",
    "    intensity_values = intensity_values[mask]\n",
    "    std_values = std_values[mask]\n",
    "    \n",
    "    plt.errorbar(mz_values, intensity_values, yerr=1.96*std_values, \n",
    "                 fmt='o', alpha=0.7, label=col if len(mz_values) > 0 else None)\n",
    "\n",
    "# Plot y ions\n",
    "y_cols = [col for col in frag_mz.columns if col.startswith('y_')]\n",
    "for col in y_cols:\n",
    "    mz_values = frag_mz[col].values\n",
    "    intensity_values = frag_intensity_mean[col].values\n",
    "    std_values = frag_intensity_std[col].values\n",
    "    \n",
    "    # Filter out zero m/z values\n",
    "    mask = mz_values > 0\n",
    "    mz_values = mz_values[mask]\n",
    "    intensity_values = intensity_values[mask]\n",
    "    std_values = std_values[mask]\n",
    "    \n",
    "    plt.errorbar(mz_values, intensity_values, yerr=1.96*std_values, \n",
    "                 fmt='s', alpha=0.7, label=col if len(mz_values) > 0 else None)\n",
    "\n",
    "plt.xlabel('m/z')\n",
    "plt.ylabel('Relative Intensity')\n",
    "plt.title(f'MS2 Spectrum with Uncertainty for {ms2_df.iloc[peptide_idx][\"sequence\"]}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've implemented and demonstrated two approaches for uncertainty quantification in AlphaPeptDeep models:\n",
    "\n",
    "1. **Monte Carlo Dropout**: By enabling dropout at inference time, we can sample from the approximate posterior distribution of the model parameters. This approach is computationally efficient as it only requires a single trained model.\n",
    "\n",
    "2. **Deep Ensembles**: By training multiple models with different random initializations, we can capture model uncertainty. This approach is more computationally expensive but can provide more reliable uncertainty estimates.\n",
    "\n",
    "We've evaluated these approaches using metrics such as Prediction Interval Coverage Probability (PICP) and Mean Prediction Interval Width (MPIW), and visualized the results using calibration curves and error bars.\n",
    "\n",
    "These uncertainty quantification methods can be applied to all prediction tasks in AlphaPeptDeep, including RT, CCS, and MS2 prediction. They provide valuable information about the confidence of the model's predictions, which can be used to improve decision-making in downstream analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
